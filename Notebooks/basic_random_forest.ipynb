{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab nbagg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/storage/new_york_311/Notebooks/utils.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "- We are only dealing with 10% of the data for memory and time purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('../../data/new_york_311')\n",
    "data_path = data_folder / 'data_311.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (32,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "orig_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Certain Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main columns that would be useful is resolution description. Depending on the scenario or application this could be considered to have leakage. However, if we were to include it in the features, we can try several approaches:\n",
    "- Drop stop words\n",
    "- Convert words to vectors using word2vec, take the average of the vectors\n",
    "- Sklearn HashingVectorizer\n",
    "- Use Tf-Idf to find important words, use word2vec on top N words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'unique_key',                     # If interested in finding hidden leakage, investigate this column\n",
    "    'agency_name',                    # Redundant to agancy\n",
    "    'descriptor',                     # This gives away the complaint type, leakage\n",
    "    'incident_address',               # Didnt want street numbers\n",
    "    'bbl',                            # We already have enough goelocaions\n",
    "    'location',                       # Redundant to Lat and Lng\n",
    "    'resolution_action_updated_date', # caused errors\n",
    "    # Maybe Use if there is time\n",
    "    'resolution_description'         # Probably very useful but not enough time to use and maybe leakage\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.drop_useless_cols(orig_data, drop_cols)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Rows\n",
    "Only use rows that contain the 130 complaint types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.filter_rows(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Continuous, Categorical, Date Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = [\n",
    "    'x_coordinate_state_plane',\n",
    "    'y_coordinate_state_plane',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'time_to_close',\n",
    "    'due_len',\n",
    "    'time_over'\n",
    "]\n",
    "cat_cols = [\n",
    "    'agency',\n",
    "    'borough',\n",
    "    'location_type',\n",
    "    'incident_zip',\n",
    "    'street_name',\n",
    "    'cross_street_1',\n",
    "    'cross_street_2',\n",
    "    'intersection_street_1',\n",
    "    'intersection_street_2',\n",
    "    'address_type',\n",
    "    'city',\n",
    "    'landmark',\n",
    "    'facility_type',\n",
    "    'status',\n",
    "    'community_board',\n",
    "    'open_data_channel_type',\n",
    "    'park_facility_name',\n",
    "    'park_borough',\n",
    "    'vehicle_type',\n",
    "    'taxi_company_borough',\n",
    "    'taxi_pick_up_location',\n",
    "    'bridge_highway_name',\n",
    "    'bridge_highway_direction',\n",
    "    'road_ramp',\n",
    "    'bridge_highway_segment',\n",
    "]\n",
    "date_cols = [\n",
    "    'created_date',\n",
    "    'closed_date',\n",
    "    'due_date',\n",
    "]\n",
    "dep_var = ['complaint_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Date Columns\n",
    "- Add columns for time elapsed between dates\n",
    "- Add features for day of week, end of year, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " df, cont_cols, cat_cols = utils.process_date_cols(df, cont_cols, cat_cols, date_cols, dep_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Continuous Columns\n",
    "- Fill in missing values with the median of the column, then add another feature that idicates which rows had missing values\n",
    "- Normalize Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, cont_cols, cat_cols = utils.process_cont_cols(df, cont_cols, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Categorical Columns\n",
    "- Ordinal Encoding for columns with number of classes greater than 20\n",
    "- One hot encoding for columns with number of classes 20 or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, cat_cols, label_encoders = utils.process_cat_cols(df, cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataframe into Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, label_encoders = utils.split_target(df, dep_var[0], label_encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Random Forest Classifier and Compare to Naive Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 33\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "clf = RandomForestClassifier(max_depth=4, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=4, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=33, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_x)\n",
    "correct_loc = np.where(preds == test_y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_correct = len(correct_loc) / len(preds)\n",
    "perc_correct_naive = len(np.where(test_y == mode(test_y)[0][0])[0]) / len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "Random Forest: 0.4234, Mode: 0.0819\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:')\n",
    "print(f'Random Forest: {round(perc_correct, 4)}, Mode: {round(perc_correct_naive, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step \n",
    "- Find the hyperparameters using a randomized search like below\n",
    "- This uses k fold cross validation, which can cause problems when trying to predict the future\n",
    "- Use randomized search with k fold to find the right parameters, then train a model using the same parameters on data 2010 - 2018 and validate on 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_random = RandomizedSearchCV(\n",
    "#     estimator = clf, \n",
    "#     param_distributions = random_grid, \n",
    "#     n_iter = 10,                            # number of random trials \n",
    "#     cv = 3,                                 # K in k fold\n",
    "#     verbose=10,                             # How much updating and printing\n",
    "#     random_state=seed, \n",
    "#     n_jobs = -1\n",
    "# )\n",
    "# rf_random.fit(train_x, train_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
